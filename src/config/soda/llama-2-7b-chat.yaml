# @package _global_

defaults:
  - data

model:
  model_name_or_path: meta-llama/Llama-2-7b-chat-hf

training:
  fp16: true
  fp16_full_eval: true

  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32

  per_device_eval_batch_size: 2
  generation_max_length: 768  # This includes the prompt length.
  
  learning_rate: 3.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.0

  peft: true
  peft_type: lora
  lora_r: 16
  target_modules: q_proj,k_proj,v_proj,o_proj

  logging_steps: 50
  save_steps: 2000
  eval_steps: 2000

  save_total_limit: 2
  max_steps: 8000
