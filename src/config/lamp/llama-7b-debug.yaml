# @package _global_

model:
  model_name_or_path: /storage/shared/janghyun/llama-7b-hf

data:
  max_eval_samples: 1

training:
  fp16: true
  fp16_full_eval: true

  per_device_train_batch_size: 1
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 128
  generation_max_length: 1280  # This includes the prompt length.
  
  learning_rate: 3.0e-4
  lr_scheduler_type: constant

  peft: true
  peft_type: lora
  lora_r: 8
  target_modules: q_proj,k_proj,v_proj,o_proj

  logging_steps: 1
  eval_steps: 20
  save_steps: 20
  max_steps: 20

wandb:
  name: debug
  log: false